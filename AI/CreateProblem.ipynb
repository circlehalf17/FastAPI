{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"lLpSHztm1Qwk"},"outputs":[],"source":["# 1. ÌïÑÏöîÌïú Ìå®ÌÇ§ÏßÄ ÏÑ§Ïπò\n","!pip install fastapi pyngrok uvicorn transformers accelerate nest_asyncio --quiet"]},{"cell_type":"code","source":["# 2. Î™®Îç∏ Î°úÎî©\n","from transformers import GPTNeoForCausalLM, GPT2Tokenizer\n","import torch\n","\n","model_name = \"EleutherAI/gpt-neo-1.3B\"\n","tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n","model = GPTNeoForCausalLM.from_pretrained(model_name).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"metadata":{"id":"IMAxICaQvIb7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Î™®Îç∏Í≥º ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä Î°úÎìú\n","model_name = \"EleutherAI/gpt-neo-1.3B\"\n","tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n","model = GPTNeoForCausalLM.from_pretrained(model_name).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"metadata":{"id":"53O7YKLGgTHO"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pwHgIP_kWtTg"},"outputs":[],"source":["# 3. ÌÖçÏä§Ìä∏ ÏÉùÏÑ± Ìï®Ïàò (ÏµúÏ†ÅÌôî Î≤ÑÏ†Ñ)\n","'''\n","def generate_text(prompt, max_length=512):\n","    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n","    output_ids = model.generate(\n","        input_ids,\n","        do_sample=True,\n","        temperature=0.9,               # Ï∞ΩÏùòÏÑ± ÎÜíÏûÑ\n","        top_p=0.95,                   # nucleus sampling\n","        repetition_penalty=1.1,       # Î∞òÎ≥µ Î∞©ÏßÄ\n","        max_length=max_length,\n","        pad_token_id=tokenizer.eos_token_id,\n","    )\n","    return tokenizer.decode(output_ids[0], skip_special_tokens=True)\n","'''"]},{"cell_type":"code","source":["# 4. Ï†ïÍ∑úÌëúÌòÑÏãù ÌååÏã± Ìï®Ïàò Ï†ïÏùò\n","import re\n","\n","def parse_problems_from_text(text):\n","    pattern = r\"Question\\s*[:Ôºö]\\s*(.*?)\\s*Answer\\s*[:Ôºö]\\s*(.*?)(?=\\nQuestion|\\Z)\"\n","    matches = re.findall(pattern, text, re.DOTALL)\n","    return [{\"Question\": q.strip(), \"Answer\": a.strip()} for q, a in matches]"],"metadata":{"id":"bdLcf0tezl38"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"olhZL-euhOFI"},"outputs":[],"source":["# 5. FastAPI app ÏÑ§Ï†ï\n","from fastapi import FastAPI\n","from pydantic import BaseModel\n","from transformers import pipeline\n","\n","app = FastAPI()\n","generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n","\n","class ProblemRequest(BaseModel):\n","    question_type: str\n","    topic: str\n","    count: int\n","\n","@app.post(\"/generate\")\n","def generate_problems(req: ProblemRequest):\n","    prompt = f\"\"\"\n","    Generate {req.count} {req.question_type} questions about {req.topic}.\n","\n","    Question: ...\n","    Answer: ...\n","    (repeat {req.count} times)\n","    \"\"\"\n","\n","    result = generator(prompt, max_length=400, do_sample=True, temperature=0.8, top_p=0.9, repetition_penalty=1)\n","    output = result[0][\"generated_text\"]\n","    print(\"\\nGPT Output:\\n\", output)\n","\n","    try:\n","        problems = parse_problems_from_text(output)\n","        if not problems:\n","            raise ValueError(\"No questions could be parsed.\")\n","    except Exception as e:\n","        problems = [{\"Question\": output if isinstance(output, str) else str(output), \"Answer\": f\"‚ö†Ô∏è Parsing error: {str(e)}\"}]\n","\n","    return {\n","        \"results\": problems,\n","        \"raw_output\": output\n","    }"]},{"cell_type":"code","source":["# 6. Run ngrok + uvicorn server (Colab only)\n","from pyngrok import ngrok\n","import nest_asyncio\n","import uvicorn\n","\n","ngrok.set_auth_token(\"2v1Fi5CEzLumREBpheNMIIepRlM_7uLFbq5PGe81hmEZiAe9K\")  # Replace with your ngrok token\n","ngrok.kill()  # Kill previous tunnels\n","\n","public_url = ngrok.connect(3000)\n","print(\"üîó Public URL:\", public_url.public_url)\n","\n","nest_asyncio.apply()\n","uvicorn.run(app, host=\"0.0.0.0\", port=3000)\n"],"metadata":{"id":"345RPuj4jYpU","executionInfo":{"status":"ok","timestamp":1743417758517,"user_tz":-540,"elapsed":483328,"user":{"displayName":"Ïù¥ÏõêÎπà","userId":"01980967024098412535"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"860ba751-3efb-4a5e-98e3-eccb216e86f1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["üîó Public URL: https://9773-34-73-244-5.ngrok-free.app\n"]},{"output_type":"stream","name":"stderr","text":["INFO:     Started server process [584]\n","INFO:     Waiting for application startup.\n","INFO:     Application startup complete.\n","INFO:     Uvicorn running on http://0.0.0.0:3000 (Press CTRL+C to quit)\n"]},{"output_type":"stream","name":"stdout","text":["INFO:     155.230.85.158:0 - \"GET /docs HTTP/1.1\" 200 OK\n","INFO:     155.230.85.158:0 - \"GET /openapi.json HTTP/1.1\" 200 OK\n"]},{"output_type":"stream","name":"stderr","text":["Device set to use cpu\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","ERROR:asyncio:Task exception was never retrieved\n","future: <Task finished name='Task-17' coro=<Server.serve() done, defined at /usr/local/lib/python3.11/dist-packages/uvicorn/server.py:68> exception=KeyboardInterrupt()>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/main.py\", line 579, in run\n","    server.run()\n","  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\", line 66, in run\n","    return asyncio.run(self.serve(sockets=sockets))\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 30, in run\n","    return loop.run_until_complete(task)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 92, in run_until_complete\n","    self._run_once()\n","  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 133, in _run_once\n","    handle._run()\n","  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n","    self._context.run(self._callback, *self._args)\n","  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 360, in __wakeup\n","    self.__step()\n","  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n","    result = coro.send(None)\n","             ^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\", line 69, in serve\n","    with self.capture_signals():\n","  File \"/usr/lib/python3.11/contextlib.py\", line 144, in __exit__\n","    next(self.gen)\n","  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\", line 330, in capture_signals\n","    signal.raise_signal(captured_signal)\n","KeyboardInterrupt\n"]},{"output_type":"stream","name":"stdout","text":["\n","GPT Output:\n"," \n","    Generate 2 Basic questions about Math.\n","\n","    Question: ...\n","    Answer: ...\n","    (repeat 2 times)\n","        Question:...\n","        Answer:...\n","        (repeat 2 times)\n","            Question:...\n","            Answer:...\n","\n","    Answer:...\n","    (repeat 2 times)\n","        Question:...\n","        Answer:...\n","        (repeat 2 times)\n","            Question:...\n","            Answer:...\n","\n","    Question:...\n","    Answer:...\n","    (repeat 2 times)\n","        Question:...\n","        Answer:...\n","        (repeat 2 times)\n","            Question:...\n","            Answer:...\n","\n","    Question:...\n","    Answer:...\n","    (repeat 2 times)\n","        Question:...\n","        Answer:...\n","        (repeat 2 times)\n","            Question:...\n","            Answer:...\n","\n","A:\n","\n","This is an implementation of the Fibonacci sequence. It uses an iterative implementation of the recurrence:\n","\n","fib(n) = 1 + f(n - 1\n","INFO:     155.230.85.158:0 - \"POST /generate HTTP/1.1\" 200 OK\n"]},{"output_type":"stream","name":"stderr","text":["INFO:     Shutting down\n","INFO:     Waiting for application shutdown.\n","INFO:     Application shutdown complete.\n","INFO:     Finished server process [584]\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"34ePSeakBrwR"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"mount_file_id":"1h1WVOwUWaOeMV3nEbSg-opXZej90aLPC","authorship_tag":"ABX9TyPJTXYMNCvwXlctB3qud8q7"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}